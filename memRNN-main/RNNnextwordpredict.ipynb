{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNnextwordpredict",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc2UNzDPg7EA"
      },
      "source": [
        "import csv\n",
        "import itertools\n",
        "import operator\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sys\n",
        "from datetime import datetime\n",
        "!pip3 install utils\n",
        "from utils import *\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "# Download NLTK model data (you need to do this once)\n",
        "nltk.download(\"book\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rho1Juvihj9_"
      },
      "source": [
        "vocabulary_size = 8000\n",
        "unknown_token = \"UNKNOWN_TOKEN\"\n",
        "sentence_start_token = \"SENTENCE_START\"\n",
        "sentence_end_token = \"SENTENCE_END\"\n",
        "\n",
        "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
        "print(\"Reading CSV file\")\n",
        "with open('reddit-comments-2015-08.csv', 'r', encoding='latin1') as f:\n",
        "    reader = csv.reader((line.replace('\\0','') for line in f), skipinitialspace=True)\n",
        "    next(reader)\n",
        "    # Split full comments into sentences\n",
        "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
        "    # Append SENTENCE_START and SENTENCE_END\n",
        "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
        "print(\"Parsed %d sentences.\" % (len(sentences)))\n",
        "    \n",
        "# Tokenize the sentences into words\n",
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Count the word frequencies\n",
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
        "\n",
        "# Get the most common words and build index_to_word and word_to_index vectors\n",
        "vocab = word_freq.most_common(vocabulary_size-1)\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
        "\n",
        "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
        "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
        "\n",
        "# Replace all words not in our vocabulary with the unknown token\n",
        "for i, sent in enumerate(tokenized_sentences):\n",
        "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
        "\n",
        "print(\"\\nExample sentence: '%s'\" % sentences[1])\n",
        "print(\"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nd-pomChymR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05372a20-5aa0-479e-e63e-7f14324c578b"
      },
      "source": [
        "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
        "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
        "x_example, y_example = X_train[18], y_train[18]\n",
        "print (\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
        "print (\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "SENTENCE_START what are n't you understanding about this ? !\n",
            "[0, 52, 28, 16, 10, 853, 54, 26, 35, 70]\n",
            "\n",
            "y:\n",
            "what are n't you understanding about this ? ! SENTENCE_END\n",
            "[52, 28, 16, 10, 853, 54, 26, 35, 70, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-x2_z0jh4r2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63f15546-8d25-40b4-de2a-6a56bdca9b2d"
      },
      "source": [
        "class RNNNumpy:\n",
        "    \n",
        "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
        "        # Assign instance variables\n",
        "        self.word_dim = word_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bptt_truncate = bptt_truncate\n",
        "        # Randomly initialize the network parameters\n",
        "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
        "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
        "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
        "\n",
        "def softmax(x):\n",
        "    xt = np.exp(x - np.max(x))\n",
        "    return xt / np.sum(xt)\n",
        "\n",
        "def forward_propagation(self, x):\n",
        "    # The total number of time steps\n",
        "    T = len(x)\n",
        "    # During forward propagation we save all hidden states in s because need them later.\n",
        "    # We add one additional element for the initial hidden, which we set to 0\n",
        "    s = np.zeros((T + 1, self.hidden_dim))\n",
        "    s[-1] = np.zeros(self.hidden_dim)\n",
        "    # The outputs at each time step. Again, we save them for later.\n",
        "    o = np.zeros((T, self.word_dim))\n",
        "    # For each time step...\n",
        "    for t in np.arange(T):\n",
        "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
        "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
        "        o[t] = softmax(self.V.dot(s[t]))\n",
        "    return [o, s]\n",
        "\n",
        "RNNNumpy.forward_propagation = forward_propagation\n",
        "\n",
        "def predict(self, x):\n",
        "    # Perform forward propagation and return index of the highest score\n",
        "    o, s = self.forward_propagation(x)\n",
        "    return np.argmax(o, axis=1)\n",
        "\n",
        "RNNNumpy.predict = predict\n",
        "\n",
        "np.random.seed(10)\n",
        "model = RNNNumpy(vocabulary_size)\n",
        "o, s = model.forward_propagation(X_train[10])\n",
        "print(o.shape)\n",
        "print(o)\n",
        "predictions = model.predict(X_train[10])\n",
        "print(predictions.shape)\n",
        "print(predictions)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(16, 8000)\n",
            "[[0.00012408 0.0001244  0.00012603 ... 0.00012515 0.00012488 0.00012508]\n",
            " [0.00012448 0.00012615 0.00012402 ... 0.00012514 0.00012425 0.00012528]\n",
            " [0.00012466 0.00012539 0.00012453 ... 0.0001251  0.00012575 0.00012581]\n",
            " ...\n",
            " [0.00012505 0.00012493 0.00012428 ... 0.00012581 0.00012497 0.00012489]\n",
            " [0.00012491 0.0001252  0.00012491 ... 0.00012442 0.00012416 0.0001257 ]\n",
            " [0.00012462 0.0001246  0.00012513 ... 0.00012453 0.00012502 0.00012456]]\n",
            "(16,)\n",
            "[1284 6751 6465 1387 2613 6546 4395 6455 7299 6722 6892 5354 6119 5401\n",
            " 2147 5027]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbJYebkiuJ5g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb9f5fe-eeca-45e4-8823-8c57b78969c0"
      },
      "source": [
        "'''\n",
        "the loss is defined as\n",
        "L(y, o) = -\\frac{1}{N} \\sum_{n \\in N} y_n log(o_n)\n",
        "'''\n",
        "def calculate_total_loss(self, x, y):\n",
        "    L = 0\n",
        "    # for each sentence ...\n",
        "    for i in np.arange(len(y)):\n",
        "        o, s = self.forward_propagation(x[i])\n",
        "        # we only care about our prediction of the \"correct\" words\n",
        "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
        "        # add to the loss based on how off we were\n",
        "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
        "    return L\n",
        "\n",
        "def calculate_loss(self, x, y):\n",
        "    # divide the total loss by the number of training examples\n",
        "    N = np.sum((len(y_i) for y_i in y))\n",
        "    return self.calculate_total_loss(x, y)/N\n",
        "\n",
        "RNNNumpy.calculate_total_loss = calculate_total_loss\n",
        "RNNNumpy.calculate_loss = calculate_loss\n",
        "\n",
        "print(\"Expected Loss for random prediction: %f\" % np.log(vocabulary_size))\n",
        "print(\"Actual loss: %f\" % model.calculate_loss(X_train[:1000], y_train[:1000]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expected Loss for random prediction: 8.987197\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Actual loss: 8.987368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRzosPFAwoSX"
      },
      "source": [
        "## 3. BPTT\n",
        "'''\n",
        "1. we nudge the parameters into a direction that reduces the error. the direction is given by the gradient of the loss: \\frac{\\partial L}{\\partial U}, \n",
        "\\frac{\\partial L}{\\partial V}, \\frac{\\partial L}{\\partial W}\n",
        "2. we also need learning rate: which indicated how big of a step we want to make in each direction\n",
        "Q: how to optimize SGD using batching, parallelism and adaptive learning rates.\n",
        "\n",
        "RNN BPTT: because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the\n",
        "current time step, but also the previous time steps.\n",
        "'''\n",
        "def bptt(self, x, y):\n",
        "    T = len(y)\n",
        "    # perform forward propagation\n",
        "    o, s = self.forward_propagation(x)\n",
        "    # we will accumulate the gradients in these variables\n",
        "    dLdU = np.zeros(self.U.shape)\n",
        "    dLdV = np.zeros(self.V.shape)\n",
        "    dLdW = np.zeros(self.W.shape)\n",
        "    delta_o = o\n",
        "    delta_o[np.arange(len(y)), y] -= 1   # it is y_hat - y\n",
        "    # for each output backwards ...\n",
        "    for t in np.arange(T):\n",
        "        dLdV += np.outer(delta_o[t], s[t].T)# at time step t, shape is word_dim * hidden_dim\n",
        "        # initial delta calculation\n",
        "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
        "        # backpropagation through time (for at most self.bptt_truncate steps)\n",
        "        # given time step t, go back from time step t, to t-1, t-2, ...\n",
        "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
        "            # print(\"Backprogation step t=%d bptt step=%d\" %(t, bptt_step))\n",
        "            dLdW += np.outer(delta_t, s[bptt_step - 1])\n",
        "            dLdU[:, x[bptt_step]] += delta_t\n",
        "            # update delta for next step\n",
        "            dleta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1]**2)\n",
        "    return [dLdU, dLdV, dLdW]\n",
        "\n",
        "RNNNumpy.bptt = bptt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTueyV3r3V6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5da5e48-7843-4d3c-cf89-783b3b29cf74"
      },
      "source": [
        "## 4. SGD implementation\n",
        "'''\n",
        "two step:\n",
        "1. calculate the gradients and perform the updates for one batch\n",
        "2. loop through the training set and adjust the learning rate\n",
        "'''\n",
        "### 4.1. perform one step of SGD\n",
        "def numpy_sgd_step(self, x, y, learning_rate):\n",
        "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
        "    self.U -= learning_rate * dLdU\n",
        "    self.V -= learning_rate * dLdV\n",
        "    self.W -= learning_rate * dLdW\n",
        "RNNNumpy.sgd_step = numpy_sgd_step\n",
        "\n",
        "### 4.2. outer SGD loop\n",
        "'''\n",
        " - model: \n",
        " - X_train:\n",
        " - y_train:\n",
        " - learning_rate:\n",
        " - nepoch:\n",
        " - evaluate loss_after:\n",
        "'''\n",
        "def train_with_sgd(model, X_train, y_train, learning_rate = 0.005, nepoch = 100, evaluate_loss_after = 5):\n",
        "    # keep track of the losses so that we can plot them later\n",
        "    losses = []\n",
        "    num_examples_seen = 0\n",
        "    for epoch in range(nepoch):\n",
        "        # optionally evaluate the loss\n",
        "        if (epoch % evaluate_loss_after == 0):\n",
        "            loss = model.calculate_loss(X_train, y_train)\n",
        "            losses.append((num_examples_seen, loss))\n",
        "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            print(\"%s: loss after num_examples_seen=%d epoch=%d: %f\" %(time, num_examples_seen, epoch, loss))\n",
        "            # adjust the learning rate if loss increases\n",
        "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
        "                learning_rate = learning_rate * 0.5\n",
        "                print(\"setting learning rate to %f\" %(learning_rate))\n",
        "            sys.stdout.flush()\n",
        "        # for each training example...\n",
        "        for i in range(len(y_train)):\n",
        "            # one sgd step\n",
        "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
        "            #print(\"model.U\", model.U)\n",
        "            num_examples_seen += 1\n",
        "\n",
        "np.random.seed(10)\n",
        "model = RNNNumpy(vocabulary_size)\n",
        "%timeit model.sgd_step(X_train[10], y_train[10], 0.005)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 loops, best of 3: 87.4 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpdQQnSu35uw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9feb007-f526-4296-e7ab-2777963b0322"
      },
      "source": [
        "np.random.seed(10)\n",
        "model = RNNNumpy(vocabulary_size)\n",
        "print(model.U)\n",
        "losses = train_with_sgd(model, X_train[1:100], y_train[1:100], nepoch = 10, evaluate_loss_after = 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.00606691 -0.01071631  0.00298847 ... -0.01015375 -0.00477044\n",
            "   0.00578017]\n",
            " [ 0.00944173 -0.0074726   0.00688785 ... -0.01038716 -0.00536007\n",
            "  -0.00084724]\n",
            " [ 0.00682407 -0.00349747  0.0099399  ...  0.00208931  0.00112797\n",
            "  -0.00020396]\n",
            " ...\n",
            " [-0.00758435  0.0015517  -0.01051649 ...  0.00565983 -0.00919381\n",
            "  -0.00272508]\n",
            " [ 0.0014529   0.00681475  0.00820687 ...  0.00290203 -0.00640613\n",
            "   0.00775899]\n",
            " [-0.0090378  -0.00120979  0.00874173 ... -0.00166648 -0.00791153\n",
            "  -0.00890975]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-11-05 05:15:42: loss after num_examples_seen=0 epoch=0: 8.987353\n",
            "2020-11-05 05:15:53: loss after num_examples_seen=99 epoch=1: 8.973855\n",
            "2020-11-05 05:16:05: loss after num_examples_seen=198 epoch=2: 8.943834\n",
            "2020-11-05 05:16:16: loss after num_examples_seen=297 epoch=3: 6.868403\n",
            "2020-11-05 05:16:28: loss after num_examples_seen=396 epoch=4: 6.302610\n",
            "2020-11-05 05:16:39: loss after num_examples_seen=495 epoch=5: 6.049862\n",
            "2020-11-05 05:16:50: loss after num_examples_seen=594 epoch=6: 5.896365\n",
            "2020-11-05 05:17:02: loss after num_examples_seen=693 epoch=7: 5.790338\n",
            "2020-11-05 05:17:13: loss after num_examples_seen=792 epoch=8: 5.712210\n",
            "2020-11-05 05:17:24: loss after num_examples_seen=891 epoch=9: 5.651878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vIlJDkNCFPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bbdc52a-19e6-4cc0-fdb6-e8f3e5a4fefa"
      },
      "source": [
        "print(model.U)\n",
        "def generate_sentence(model):\n",
        "    # We start the sentence with the start token\n",
        "    new_sentence = [word_to_index[sentence_start_token]]\n",
        "    # Repeat until we get an end token\n",
        "    while not new_sentence[-1] == word_to_index[sentence_end_token]:\n",
        "        next_word_probs,s = model.forward_propagation(new_sentence)\n",
        "        sampled_word = word_to_index[unknown_token]\n",
        "        # We don't want to sample unknown words\n",
        "        while sampled_word == word_to_index[unknown_token]:\n",
        "            samples = np.random.multinomial(1, next_word_probs[-1])\n",
        "            sampled_word = np.argmax(samples)\n",
        "            print(sampled_word)\n",
        "        new_sentence.append(sampled_word)\n",
        "        if len(new_sentence) >= 20:\n",
        "          break\n",
        "    sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
        "    print(sentence_str)\n",
        "\n",
        "num_sentences = 1\n",
        "senten_min_length = 7\n",
        "\n",
        "for i in range(num_sentences):\n",
        "  generate_sentence(model)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.16155555 -0.01071631  0.1092198  ... -0.01015375 -0.00477044\n",
            "   0.11427511]\n",
            " [-0.01563373 -0.0074726  -0.07312643 ... -0.01038716 -0.00536007\n",
            "   0.02745094]\n",
            " [-0.34838702 -0.00349747  0.07752491 ...  0.00208931  0.00112797\n",
            "  -0.07091841]\n",
            " ...\n",
            " [-0.14211439  0.0015517  -0.11025854 ...  0.00565983 -0.00919381\n",
            "  -0.05688476]\n",
            " [-0.45786422  0.00681475  0.05582244 ...  0.00290203 -0.00640613\n",
            "  -0.05426409]\n",
            " [-0.49525312 -0.00120979  0.01973171 ... -0.00166648 -0.00791153\n",
            "  -0.1019795 ]]\n",
            "3271\n",
            "6\n",
            "73\n",
            "16\n",
            "7999\n",
            "66\n",
            "642\n",
            "98\n",
            "4828\n",
            "16\n",
            "4442\n",
            "123\n",
            "207\n",
            "3720\n",
            "2615\n",
            "3\n",
            "2\n",
            "46\n",
            "163\n",
            "6436\n",
            "['angle', 'i', 'no', \"n't\", 'will', 'allowed', 'good', 'transport', \"n't\", 'arts', 'into', 'used', 'prepare', 'frustrating', 'the', '.', 'at', 'our']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}